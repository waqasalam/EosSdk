{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waqasalam/EosSdk/blob/master/RAG_Pipeline_MVP_for_Bug_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, ensure you have the necessary libraries installed:\n",
        "# pip install langchain langchain-openai langchain-community \"langgraph[hub]\" \"arize-phoenix[all]\" chromadb\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "from typing import List, Literal, TypedDict\n",
        "\n",
        "# LangChain/LangGraph imports\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "# Arize Phoenix imports\n",
        "import phoenix as px\n",
        "from phoenix.trace import LangChainSpanEvaluator, SpanEvaluator\n",
        "\n",
        "# Initialize Phoenix for tracing\n",
        "# This will launch the Phoenix UI, typically at http://localhost:6006\n",
        "session = px.launch_app()\n",
        "\n",
        "# --- Configuration ---\n",
        "# IMPORTANT: Ensure your OPENAI_API_KEY is set as an environment variable.\n",
        "# Example (do not hardcode in production):\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    print(\"WARNING: OPENAI_API_KEY environment variable not set. Please set it to run the LLM.\")\n",
        "    # Exit or handle gracefully if API key is critical for execution\n",
        "    # For this MVP, we'll proceed, but LLM calls will fail without the key.\n",
        "\n",
        "# --- 1. Data Collection & Preprocessing (Mock Data for MVP) ---\n",
        "\n",
        "# Mock Datadog Logs\n",
        "# In a real application, you would fetch these via Datadog's API.\n",
        "datadog_logs = [\n",
        "    {\n",
        "        \"timestamp\": \"2023-05-15T10:00:00Z\",\n",
        "        \"service\": \"authentication-service\",\n",
        "        \"level\": \"ERROR\",\n",
        "        \"message\": \"Failed to authenticate user 'john.doe': Invalid credentials.\",\n",
        "        \"log_id\": \"log-001\"\n",
        "    },\n",
        "    {\n",
        "        \"timestamp\": \"2023-05-15T10:05:30Z\",\n",
        "        \"service\": \"order-processing-service\",\n",
        "        \"level\": \"WARN\",\n",
        "        \"message\": \"High latency detected for database query 'get_orders_by_user'. Duration: 2500ms.\",\n",
        "        \"log_id\": \"log-002\"\n",
        "    },\n",
        "    {\n",
        "        \"timestamp\": \"2023-05-15T10:10:15Z\",\n",
        "        \"service\": \"payment-gateway\",\n",
        "        \"level\": \"ERROR\",\n",
        "        \"message\": \"Stripe API call failed: Payment declined. Transaction ID: XYZ123.\",\n",
        "        \"log_id\": \"log-003\"\n",
        "    },\n",
        "    {\n",
        "        \"timestamp\": \"2023-05-15T10:12:00Z\",\n",
        "        \"service\": \"authentication-service\",\n",
        "        \"level\": \"ERROR\",\n",
        "        \"message\": \"Rate limit exceeded for login attempts from IP: 192.168.1.10. User: 'jane.doe'.\",\n",
        "        \"log_id\": \"log-004\"\n",
        "    },\n",
        "    {\n",
        "        \"timestamp\": \"2023-05-16T11:00:00Z\",\n",
        "        \"service\": \"inventory-service\",\n",
        "        \"level\": \"ERROR\",\n",
        "        \"message\": \"Negative stock detected for product ID: P123. Current stock: -5.\",\n",
        "        \"log_id\": \"log-005\"\n",
        "    },\n",
        "]\n",
        "\n",
        "# Mock Jira Issues\n",
        "# In a real application, you would fetch these via Jira's API.\n",
        "jira_issues = [\n",
        "    {\n",
        "        \"issue_id\": \"JIRA-101\",\n",
        "        \"summary\": \"Authentication failure for invalid credentials\",\n",
        "        \"description\": \"Users are reporting issues logging in with correct credentials. Logs show 'Invalid credentials' error from authentication-service.\",\n",
        "        \"status\": \"Open\",\n",
        "        \"priority\": \"High\"\n",
        "    },\n",
        "    {\n",
        "        \"issue_id\": \"JIRA-102\",\n",
        "        \"summary\": \"Slow order processing due to DB latency\",\n",
        "        \"description\": \"Order processing times have increased. Datadog logs indicate high latency for 'get_orders_by_user' query.\",\n",
        "        \"status\": \"In Progress\",\n",
        "        \"priority\": \"Medium\"\n",
        "    },\n",
        "    {\n",
        "        \"issue_id\": \"JIRA-103\",\n",
        "        \"summary\": \"Stripe payment declines for some transactions\",\n",
        "        \"description\": \"Some payments are being declined via Stripe. Logs show 'Payment declined' errors.\",\n",
        "        \"status\": \"Open\",\n",
        "        \"priority\": \"Critical\"\n",
        "    },\n",
        "    {\n",
        "        \"issue_id\": \"JIRA-104\",\n",
        "        \"summary\": \"Implement rate limiting for login attempts\",\n",
        "        \"description\": \"Multiple login attempts from single IPs are not rate-limited, leading to potential brute-force attacks.\",\n",
        "        \"status\": \"To Do\",\n",
        "        \"priority\": \"Low\"\n",
        "    },\n",
        "    {\n",
        "        \"issue_id\": \"JIRA-105\",\n",
        "        \"summary\": \"Negative stock bug in inventory service\",\n",
        "        \"description\": \"Inventory service is reporting negative stock for some products, leading to incorrect availability.\",\n",
        "        \"status\": \"Open\",\n",
        "        \"priority\": \"High\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Mock Codebase (simplified for MVP, imagine full files)\n",
        "# In a real application, you would integrate with your Git repository\n",
        "# and use more sophisticated code parsing/chunking.\n",
        "code_snippets = [\n",
        "    {\n",
        "        \"file_path\": \"auth_service/handlers.py\",\n",
        "        \"function_name\": \"authenticate_user\",\n",
        "        \"code\": \"\"\"\n",
        "def authenticate_user(username, password):\n",
        "    # ...\n",
        "    # This is where the authentication logic resides.\n",
        "    # A common bug here is incorrect password hashing or comparison.\n",
        "    if not check_password_hash(user.password_hash, password):\n",
        "        logger.error(f\"Failed to authenticate user '{username}': Invalid credentials.\")\n",
        "        return False\n",
        "    # ...\n",
        "\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"file_path\": \"order_service/db_utils.py\",\n",
        "        \"function_name\": \"get_orders_by_user\",\n",
        "        \"code\": \"\"\"\n",
        "def get_orders_by_user(user_id):\n",
        "    start_time = time.time()\n",
        "    # This query might be inefficient for large datasets or complex joins.\n",
        "    orders = db.query(Order).filter_by(user_id=user_id).all()\n",
        "    duration = (time.time() - start_time) * 1000\n",
        "    if duration > 2000: # Threshold for high latency\n",
        "        logger.warning(f\"High latency detected for database query 'get_orders_by_user'. Duration: {duration:.0f}ms.\")\n",
        "    return orders\n",
        "\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"file_path\": \"payment_service/stripe_api.py\",\n",
        "        \"function_name\": \"process_payment\",\n",
        "        \"code\": \"\"\"\n",
        "import stripe\n",
        "def process_payment(amount, token):\n",
        "    try:\n",
        "        charge = stripe.Charge.create(\n",
        "            amount=amount,\n",
        "            currency='usd',\n",
        "            source=token,\n",
        "            description='Payment for order'\n",
        "        )\n",
        "        return charge.id\n",
        "    except stripe.error.CardError as e:\n",
        "        # This catch block handles Stripe-specific card errors.\n",
        "        # Ensure all possible Stripe exceptions are handled.\n",
        "        logger.error(f\"Stripe API call failed: {e.user_message}. Transaction ID: {e.request_id}.\")\n",
        "        raise # Re-raise to propagate the error\n",
        "\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"file_path\": \"auth_service/rate_limiter.py\",\n",
        "        \"function_name\": \"check_rate_limit\",\n",
        "        \"code\": \"\"\"\n",
        "from flask_limiter import Limiter\n",
        "from flask_limiter.util import get_remote_address\n",
        "\n",
        "limiter = Limiter(\n",
        "    key_func=get_remote_address,\n",
        "    default_limits=[\"10 per minute\"] # Example rate limit\n",
        ")\n",
        "\n",
        "def check_rate_limit(ip_address, user_id):\n",
        "    # This function is defined but might not be properly integrated\n",
        "    # into the main login flow, leading to unhandled rate limit scenarios.\n",
        "    pass # Needs implementation to actually apply the limit\n",
        "\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"file_path\": \"inventory_service/stock_manager.py\",\n",
        "        \"function_name\": \"update_stock\",\n",
        "        \"code\": \"\"\"\n",
        "def update_stock(product_id, quantity_change):\n",
        "    current_stock = get_current_stock(product_id)\n",
        "    new_stock = current_stock + quantity_change\n",
        "    if new_stock < 0:\n",
        "        # This condition indicates a bug where stock can go negative.\n",
        "        # It should ideally prevent the update or trigger an immediate alert/rollback.\n",
        "        logger.error(f\"Negative stock detected for product ID: {product_id}. Current stock: {current_stock}, Change: {quantity_change}.\")\n",
        "        # For MVP, just logging the error. A real fix would prevent this state.\n",
        "    db.update_stock(product_id, new_stock)\n",
        "\"\"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Convert mock data into LangChain Document objects\n",
        "# Each document will have its content and associated metadata.\n",
        "log_documents = []\n",
        "for log in datadog_logs:\n",
        "    content = f\"Datadog Log - Service: {log['service']}, Level: {log['level']}, Message: {log['message']}\"\n",
        "    metadata = {\n",
        "        \"source\": \"datadog_log\",\n",
        "        \"timestamp\": log[\"timestamp\"],\n",
        "        \"service\": log[\"service\"],\n",
        "        \"level\": log[\"level\"],\n",
        "        \"log_id\": log[\"log_id\"]\n",
        "    }\n",
        "    log_documents.append(Document(page_content=content, metadata=metadata))\n",
        "\n",
        "jira_documents = []\n",
        "for issue in jira_issues:\n",
        "    content = f\"Jira Issue - ID: {issue['issue_id']}, Summary: {issue['summary']}, Description: {issue['description']}, Status: {issue['status']}, Priority: {issue['priority']}\"\n",
        "    metadata = {\n",
        "        \"source\": \"jira_issue\",\n",
        "        \"issue_id\": issue[\"issue_id\"],\n",
        "        \"summary\": issue[\"summary\"],\n",
        "        \"status\": issue[\"status\"],\n",
        "        \"priority\": issue[\"priority\"]\n",
        "    }\n",
        "    jira_documents.append(Document(page_content=content, metadata=metadata))\n",
        "\n",
        "code_documents = []\n",
        "for snippet in code_snippets:\n",
        "    # Include the code content directly in the document for embedding\n",
        "    content = f\"Code Snippet - File: {snippet['file_path']}, Function: {snippet['function_name']}\\n```python\\n{snippet['code']}\\n```\"\n",
        "    metadata = {\n",
        "        \"source\": \"code_snippet\",\n",
        "        \"file_path\": snippet[\"file_path\"],\n",
        "        \"function_name\": snippet[\"function_name\"]\n",
        "    }\n",
        "    code_documents.append(Document(page_content=content, metadata=metadata))\n",
        "\n",
        "# Combine all documents into a single list for the vector store\n",
        "all_documents = log_documents + jira_documents + code_documents\n",
        "\n",
        "# --- 2. Embedding and Vector Store ---\n",
        "# Initialize the OpenAI Embeddings model. This converts text into numerical vectors.\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Create a ChromaDB vector store from the documents.\n",
        "# ChromaDB is used here for simplicity as an in-memory or local file-based store.\n",
        "# For production, consider persistent and scalable vector databases like Milvus, Pinecone, etc.\n",
        "vectorstore = Chroma.from_documents(documents=all_documents, embedding=embeddings)\n",
        "\n",
        "# Create a retriever from the vector store.\n",
        "# It will fetch the top 'k' (here, 5) most semantically similar documents to a given query.\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "# --- 3. LLM Setup ---\n",
        "# Initialize the ChatOpenAI model.\n",
        "# gpt-4o-mini is chosen for its balance of capability and cost-effectiveness.\n",
        "# temperature=0 makes the LLM's responses more deterministic and factual.\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# --- 4. RAG Pipeline with LangGraph ---\n",
        "\n",
        "# Define the state for our LangGraph workflow.\n",
        "# This TypedDict specifies the data structure that will be passed between nodes.\n",
        "class AgentState(TypedDict):\n",
        "    query: str  # The initial user query or log message\n",
        "    documents: List[Document]  # List of retrieved documents (logs, Jira, code)\n",
        "    answer: str  # The final answer generated by the LLM\n",
        "\n",
        "# Define the 'retrieve_documents' node function.\n",
        "# This node is responsible for fetching relevant documents from the vector store.\n",
        "def retrieve_documents(state: AgentState):\n",
        "    print(\"\\n---NODE: Retrieving Documents---\")\n",
        "    query = state[\"query\"]\n",
        "    # Invoke the retriever to get documents similar to the query\n",
        "    documents = retriever.invoke(query)\n",
        "    print(f\"Retrieved {len(documents)} documents.\")\n",
        "    # Return the updated state with the retrieved documents\n",
        "    return {\"documents\": documents}\n",
        "\n",
        "# Define the 'generate_answer' node function.\n",
        "# This node takes the query and retrieved documents and uses the LLM to synthesize an answer.\n",
        "def generate_answer(state: AgentState):\n",
        "    print(\"\\n---NODE: Generating Answer with LLM---\")\n",
        "    query = state[\"query\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Format the retrieved documents into a single context string for the LLM.\n",
        "    context_str = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
        "\n",
        "    # Define the prompt template for the LLM.\n",
        "    # It includes a system message to guide the LLM's role and placeholders for context and query.\n",
        "    prompt_template = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are an expert AI assistant specializing in bug detection, root cause analysis, and code localization using logs, Jira issues, and code snippets.\n",
        "        Analyze the provided context carefully. Identify potential bugs, their root causes, and suggest relevant code locations.\n",
        "        If the query is about a bug, try to link it to existing Jira issues or logs.\n",
        "        Be concise and factual. If you cannot find relevant information, state that you don't have enough information.\n",
        "\n",
        "        Context:\n",
        "        {context}\"\"\"),\n",
        "        (\"user\", \"{query}\")\n",
        "    ])\n",
        "\n",
        "    # Create a LangChain chain: prompt -> LLM -> output parser.\n",
        "    chain = prompt_template | llm | StrOutputParser()\n",
        "\n",
        "    # Invoke the chain with the context and query to get the LLM's answer.\n",
        "    answer = chain.invoke({\"context\": context_str, \"query\": query})\n",
        "    print(\"LLM generated answer.\")\n",
        "    # Return the updated state with the generated answer.\n",
        "    return {\"answer\": answer}\n",
        "\n",
        "# Build the LangGraph workflow.\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Add the defined nodes to the workflow.\n",
        "workflow.add_node(\"retrieve\", retrieve_documents)\n",
        "workflow.add_node(\"generate\", generate_answer)\n",
        "\n",
        "# Set the entry point of the graph (where execution begins).\n",
        "workflow.set_entry_point(\"retrieve\")\n",
        "\n",
        "# Define the edges (transitions between nodes).\n",
        "# After retrieving documents, the workflow moves to generating the answer.\n",
        "workflow.add_edge(\"retrieve\", \"generate\")\n",
        "# After generating the answer, the workflow ends.\n",
        "workflow.add_edge(\"generate\", END)\n",
        "\n",
        "# Compile the graph into an executable application.\n",
        "app = workflow.compile()\n",
        "\n",
        "# --- 5. Arize Phoenix Integration ---\n",
        "# Phoenix automatically instruments LangChain/LangGraph calls if it's launched\n",
        "# (as done with `px.launch_app()` at the beginning).\n",
        "# You will see detailed traces of each step (retrieval, LLM invocation) in the Phoenix UI.\n",
        "\n",
        "# --- Example Usage ---\n",
        "print(\"\\n--- Running MVP Pipeline with Example Queries ---\")\n",
        "\n",
        "# Example Query 1: A log error message\n",
        "query1 = \"I see an error log 'Failed to authenticate user: Invalid credentials' from authentication-service. What could be the issue and relevant code?\"\n",
        "inputs1 = {\"query\": query1}\n",
        "print(f\"\\nProcessing Query 1: {query1}\")\n",
        "output1 = app.invoke(inputs1)\n",
        "print(f\"\\nQuery 1 Result:\\n{output1['answer']}\")\n",
        "\n",
        "# Example Query 2: A Jira issue ID\n",
        "query2 = \"We have a Jira issue JIRA-105 about negative stock. Can you find related logs or code snippets that explain this?\"\n",
        "inputs2 = {\"query\": query2}\n",
        "print(f\"\\nProcessing Query 2: {query2}\")\n",
        "output2 = app.invoke(inputs2)\n",
        "print(f\"\\nQuery 2 Result:\\n{output2['answer']}\")\n",
        "\n",
        "# Example Query 3: A code-centric query\n",
        "query3 = \"Review the 'process_payment' function in 'payment_service/stripe_api.py' for potential errors or common issues.\"\n",
        "inputs3 = {\"query\": query3}\n",
        "print(f\"\\nProcessing Query 3: {query3}\")\n",
        "output3 = app.invoke(inputs3)\n",
        "print(f\"\\nQuery 3 Result:\\n{output3['answer']}\")\n",
        "\n",
        "# Example Query 4: A general bug description\n",
        "query4 = \"We're experiencing slow responses in the order processing service. What might be the cause?\"\n",
        "inputs4 = {\"query\": query4}\n",
        "print(f\"\\nProcessing Query 4: {query4}\")\n",
        "output4 = app.invoke(inputs4)\n",
        "print(f\"\\nQuery 4 Result:\\n{output4['answer']}\")\n",
        "\n",
        "print(\"\\n--- MVP Pipeline Finished ---\")\n",
        "print(\"Check your Arize Phoenix UI for detailed traces and evaluations of these runs.\")\n",
        "print(f\"Phoenix UI URL: {session.url}\")\n",
        "\n",
        "# --- Optional: Custom Evaluation with Arize Phoenix (More advanced) ---\n",
        "# For a more detailed RAG evaluation, you'd typically define specific evaluation criteria\n",
        "# and potentially run this on a larger, labeled dataset.\n",
        "\n",
        "# Here's a placeholder for how you might define a custom evaluator if needed.\n",
        "# This would be used to programmatically evaluate the quality of the RAG responses.\n",
        "class BugDetectionEvaluator(SpanEvaluator):\n",
        "    # The 'evaluate' method is called by Phoenix for spans.\n",
        "    # You would implement logic here to assess the quality of the LLM's output\n",
        "    # based on the span's inputs, outputs, and potentially ground truth data.\n",
        "    def evaluate(self, trace_id: str, span_id: str) -> dict:\n",
        "        # Retrieve the specific span from the trace\n",
        "        span = px.get_trace_span(trace_id, span_id)\n",
        "\n",
        "        # Check if the span is an LLM call and has an 'answer' in its output\n",
        "        if span and span.operation_name == \"langchain_run\" and \"answer\" in span.output:\n",
        "            response_text = span.output[\"answer\"]\n",
        "            query_text = span.input.get(\"query\", \"\") # Get the original query\n",
        "\n",
        "            # Example: A very basic check for keywords indicating a successful bug identification\n",
        "            # In a real scenario, this would involve more sophisticated NLP or comparison\n",
        "            # against a ground truth.\n",
        "            contains_bug_keywords = any(keyword in response_text.lower() for keyword in [\"bug\", \"error\", \"issue\", \"root cause\", \"code location\"])\n",
        "\n",
        "            # You can also add checks for hallucination, relevance, etc.\n",
        "            # For instance, check if the response mentions specific services or files that were in the retrieved context.\n",
        "\n",
        "            return {\n",
        "                \"contains_bug_keywords\": contains_bug_keywords,\n",
        "                # Add more custom metrics here\n",
        "                # \"is_relevant_to_query\": True/False,\n",
        "                # \"suggests_code_fix\": True/False,\n",
        "            }\n",
        "        return {} # Return empty dict if evaluation criteria not met for this span\n",
        "\n",
        "# To use a custom evaluator, you would typically add it to Phoenix after launching:\n",
        "# px.add_evaluator(BugDetectionEvaluator())\n",
        "# Note: Activating evaluators and running evaluations usually involves more setup\n",
        "# like creating datasets and running evaluation jobs within Phoenix."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "TnPTR0jUs235"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}